{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcb78e3",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/AtfastrSlushyMaker/pl-standings-prediction-project/blob/main/notebooks/algorithms/XGBoost/xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# ⚽ Premier League Team Performance Prediction — XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e32453",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "candidate_paths = [\n",
    "    Path('data/processed/team_season_aggregated.csv'),\n",
    "    Path('../data/processed/team_season_aggregated.csv'),\n",
    "    Path('../../data/processed/team_season_aggregated.csv'),\n",
    "    Path('../../../data/processed/team_season_aggregated.csv'),\n",
    "    Path('/content/team_season_aggregated.csv')\n",
    "]\n",
    "agg_path = next((p for p in candidate_paths if p.exists()), None)\n",
    "if agg_path is None:\n",
    "    raise FileNotFoundError('team_season_aggregated.csv not found. Run preprocessing first.')\n",
    "print(f\"✅ Loading dataset: {agg_path}\")\n",
    "df = pd.read_csv(agg_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Seasons:\", sorted(df['Season'].unique()))\n",
    "print(df.head())\n",
    "\n",
    "# Feature set (inputs)\n",
    "feature_cols = [\n",
    "    'Team_encoded', 'Season_encoded',\n",
    "    'Wins', 'Draws', 'Losses',\n",
    "    'Goals_Scored', 'Goals_Conceded', 'Goal_Difference',\n",
    "    'Avg_Goals_Scored', 'Avg_Goals_Conceded',\n",
    "    'Total_Shots', 'Total_Shots_On_Target', 'Avg_Shots', 'Avg_Shots_On_Target',\n",
    "    'Shot_Accuracy', 'Clean_Sheets', 'Clean_Sheet_Rate',\n",
    "    'Yellow_Cards', 'Red_Cards', 'Fouls', 'Corners',\n",
    "    'Win_Rate', 'Home_Win_Rate', 'Away_Win_Rate', 'Points_Per_Game'\n",
    "]\n",
    "\n",
    "# Targets (performance metrics to predict)\n",
    "# Adjust this list if your dataset lacks any of these columns\n",
    "target_cols = [\n",
    "    'Points', 'Points_Per_Game', 'Wins', 'Goals_Scored', 'Goals_Conceded', 'Goal_Difference', 'Win_Rate'\n",
    "]\n",
    "\n",
    "# Validate target availability\n",
    "missing_targets = [t for t in target_cols if t not in df.columns]\n",
    "if missing_targets:\n",
    "    raise ValueError(f\"Missing target columns in dataset: {missing_targets}\")\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "print(f\"Features: {len(feature_cols)} | Targets: {len(target_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef4198",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = df['Season'] != '2024-25'\n",
    "test_mask  = df['Season'] == '2024-25'\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_test  = X[test_mask]\n",
    "\n",
    "# Build per-target y dictionaries\n",
    "y_train_dict = {t: df.loc[train_mask, t].values for t in target_cols}\n",
    "y_test_dict  = {t: df.loc[test_mask,  t].values for t in target_cols}\n",
    "\n",
    "print(f\"Training samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
    "\n",
    "# Anchor target used for hyperparameter tuning and early stopping\n",
    "anchor_target = 'Points_Per_Game' if 'Points_Per_Game' in target_cols else target_cols[0]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_dict[anchor_target], test_size=0.15, random_state=42)\n",
    "print(f\"Internal train: {len(X_tr)} | Validation: {len(X_val)} | Anchor target: {anchor_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9997db",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Randomized Search)\n",
    "We focus on key boosting + regularization parameters to balance performance and overfitting control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as st\n",
    "import time\n",
    "\n",
    "xgb_base = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',  # fast on tabular\n",
    "    booster='gbtree',\n",
    "    n_estimators=1000,   # high upper bound, early stopping will truncate\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': st.uniform(0.01, 0.25),\n",
    "    'max_depth': st.randint(3, 9),\n",
    "    'min_child_weight': st.randint(1, 8),\n",
    "    'subsample': st.uniform(0.6, 0.4),\n",
    "    'colsample_bytree': st.uniform(0.6, 0.4),\n",
    "    'gamma': st.uniform(0, 0.6),\n",
    "    'reg_lambda': st.uniform(0.5, 2.5),\n",
    "    'reg_alpha': st.uniform(0, 0.4)\n",
    "}\n",
    "\n",
    "n_iter = 40\n",
    "print(f\"Starting RandomizedSearchCV (n_iter={n_iter}) on anchor target: {anchor_target}...\")\n",
    "start = time.time()\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "search.fit(X_train, y_train_dict[anchor_target])\n",
    "elapsed = time.time() - start\n",
    "print(f\"✅ Search complete in {elapsed/60:.1f} min\")\n",
    "print(\"Best MAE:\", -search.best_score_)\n",
    "print(\"Best Params:\")\n",
    "for k,v in search.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "best_params = search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d1911",
   "metadata": {},
   "source": [
    "## Train Final Models with Early Stopping (Per Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train one XGBoost model per target using best_params and early stopping\n",
    "import xgboost\n",
    "print(f\"XGBoost version: {xgboost.__version__}\")\n",
    "\n",
    "xgb_models = {}\n",
    "train_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "for target in target_cols:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Training target: {target}\")\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        booster='gbtree',\n",
    "        n_estimators=5000,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        eval_metric='mae', \n",
    "        **best_params\n",
    "    )\n",
    "    try:\n",
    "        model.fit(\n",
    "            X_tr, y_tr if target == anchor_target else y_train_dict[anchor_target][:(len(X_tr))],  # anchor for early stopping\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=75,\n",
    "            verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        try:\n",
    "            model.fit(X_tr, y_tr if target == anchor_target else y_train_dict[anchor_target][:(len(X_tr))], eval_set=[(X_val, y_val)])\n",
    "        except TypeError:\n",
    "            model.fit(X_tr, y_tr if target == anchor_target else y_train_dict[anchor_target][:(len(X_tr))])\n",
    "    # Refit on full training data for this target using best_iteration if available\n",
    "    n_estimators_final = getattr(model, 'best_iteration', None)\n",
    "    if n_estimators_final is not None and isinstance(n_estimators_final, (int, np.integer)) and n_estimators_final > 0:\n",
    "        model_final = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            tree_method='hist',\n",
    "            booster='gbtree',\n",
    "            n_estimators=int(n_estimators_final),\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            eval_metric='mae',\n",
    "            **best_params\n",
    "        )\n",
    "    else:\n",
    "        model_final = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            tree_method='hist',\n",
    "            booster='gbtree',\n",
    "            n_estimators=best_params.get('n_estimators', 800),\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            eval_metric='mae',\n",
    "            **{k:v for k,v in best_params.items() if k != 'n_estimators'}\n",
    "        )\n",
    "    model_final.fit(X_train, y_train_dict[target])\n",
    "\n",
    "    xgb_models[target] = model_final\n",
    "    train_preds[target] = model_final.predict(X_train)\n",
    "    test_preds[target]  = model_final.predict(X_test)\n",
    "\n",
    "print(\"\\n✅ Trained models for:\", \", \".join(xgb_models.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a863d2c",
   "metadata": {},
   "source": [
    "## Evaluation Metrics (Per Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ensure target list and ground truth dicts exist even if cells ran out of order\n",
    "try:\n",
    "    target_cols\n",
    "except NameError:\n",
    "    if 'train_preds' in globals() and isinstance(train_preds, dict) and len(train_preds) > 0:\n",
    "        target_cols = list(train_preds.keys())\n",
    "        print(f\"Inferred target_cols from trained models: {target_cols}\")\n",
    "    else:\n",
    "        raise NameError(\"target_cols is not defined. Please run the data loading and training cells first.\")\n",
    "\n",
    "if 'y_train_dict' not in globals() or 'y_test_dict' not in globals():\n",
    "    if 'df' in globals() and 'train_mask' in globals() and 'test_mask' in globals():\n",
    "        y_train_dict = {t: df.loc[train_mask, t].values for t in target_cols if t in df.columns}\n",
    "        y_test_dict  = {t: df.loc[test_mask,  t].values for t in target_cols if t in df.columns}\n",
    "        missing_truth = [t for t in target_cols if t not in y_train_dict]\n",
    "        if missing_truth:\n",
    "            print(f\"⚠️ Missing ground truth for targets: {missing_truth}. They will be skipped in metrics.\")\n",
    "    else:\n",
    "        raise NameError(\"Ground truth not available. Run 'Load Data' and 'Train-Test Split' cells.\")\n",
    "\n",
    "# Filter to targets present in both predictions and truth\n",
    "available_targets = [\n",
    "    t for t in target_cols\n",
    "    if (t in train_preds and t in test_preds and t in y_train_dict and t in y_test_dict)\n",
    "]\n",
    "if not available_targets:\n",
    "    raise RuntimeError(\"No common targets available for evaluation. Train models first.\")\n",
    "\n",
    "rows = []\n",
    "for t in available_targets:\n",
    "    ytr = y_train_dict[t]\n",
    "    yte = y_test_dict[t]\n",
    "    ptr = train_preds[t]\n",
    "    pte = test_preds[t]\n",
    "    mae_tr = mean_absolute_error(ytr, ptr)\n",
    "    mae_te = mean_absolute_error(yte, pte)\n",
    "    rmse_tr = np.sqrt(mean_squared_error(ytr, ptr))\n",
    "    rmse_te = np.sqrt(mean_squared_error(yte, pte))\n",
    "    r2_tr = r2_score(ytr, ptr)\n",
    "    r2_te = r2_score(yte, pte)\n",
    "    rows.append([t, mae_tr, mae_te, rmse_tr, rmse_te, r2_tr, r2_te])\n",
    "\n",
    "metrics_df = pd.DataFrame(rows, columns=[\n",
    "    'Target','MAE_Train','MAE_Test','RMSE_Train','RMSE_Test','R2_Train','R2_Test'\n",
    "]).sort_values('MAE_Test')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"XGBOOST TEAM PERFORMANCE — METRICS BY TARGET\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False, float_format=lambda x: f\"{x:.3f}\"))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36362e",
   "metadata": {},
   "source": [
    "## Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b586522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use anchor target model for interpretability\n",
    "anchor_model = xgb_models[anchor_target]\n",
    "booster = anchor_model.get_booster()\n",
    "importance_gain = booster.get_score(importance_type='gain')\n",
    "importance_weight = booster.get_score(importance_type='weight')\n",
    "\n",
    "imp_df = pd.DataFrame([\n",
    "    (feat, importance_gain.get(feat, 0), importance_weight.get(feat, 0))\n",
    "    for feat in booster.feature_names\n",
    "], columns=['Feature','Gain','Weight']).sort_values('Gain', ascending=False)\n",
    "\n",
    "print(f\"Top 15 features by Gain for target: {anchor_target}\")\n",
    "for _, row in imp_df.head(15).iterrows():\n",
    "    bar = '█' * int((row['Gain']/max(imp_df['Gain'].max(), 1e-9))*25)\n",
    "    print(f\"{row['Feature']:25s} Gain={row['Gain']:.2f} {bar}\")\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "axes[0].barh(imp_df.head(15)['Feature'], imp_df.head(15)['Gain'], color='steelblue')\n",
    "axes[0].invert_yaxis(); axes[0].set_title(f'Top 15 (Gain) – {anchor_target}'); axes[0].set_xlabel('Gain')\n",
    "axes[1].barh(imp_df.head(15)['Feature'], imp_df.head(15)['Weight'], color='darkgreen')\n",
    "axes[1].invert_yaxis(); axes[1].set_title(f'Top 15 (Split Frequency) – {anchor_target}'); axes[1].set_xlabel('Weight')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bc4b6",
   "metadata": {},
   "source": [
    "## 2024-25 Test Season — Team Performance Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf28076",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df['Season'] == '2024-25'].copy()\n",
    "test_df['Raw_Prediction'] = pred_test\n",
    "# Lower prediction => better (closer to 1)\n",
    "ranked = test_df.sort_values('Raw_Prediction').reset_index(drop=True)\n",
    "ranked['Predicted_Position'] = range(1, len(ranked)+1)\n",
    "ranked['Error'] = ranked['Final_Position'] - ranked['Predicted_Position']\n",
    "\n",
    "show_cols = ['Final_Position','Predicted_Position','Error','Raw_Prediction','Team','Points','Wins','Goal_Difference']\n",
    "output = ranked.sort_values('Final_Position')[show_cols].copy()\n",
    "output.columns = ['Actual','Predicted','Error','RawPred','Team','Pts','W','GD']\n",
    "print(\"=\"*90) \n",
    "print(\"XGBOOST PREDICTED STANDINGS 2024-25 (Test Season)\")\n",
    "print(\"=\"*90)\n",
    "print(output.to_string(index=False))\n",
    "\n",
    "mae = output['Error'].abs().mean()\n",
    "perfect = (output['Error']==0).sum()\n",
    "within1 = (output['Error'].abs()<=1).sum()\n",
    "within2 = (output['Error'].abs()<=2).sum()\n",
    "print(\"\\nSummary:\")\n",
    "print(f\" MAE: {mae:.2f}\")\n",
    "print(f\" Perfect: {perfect}/20\")\n",
    "print(f\" ±1: {within1}/20 ({within1/20*100:.0f}%)\")\n",
    "print(f\" ±2: {within2}/20 ({within2/20*100:.0f}%)\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12e4b3",
   "metadata": {},
   "source": [
    "## 2025-26 Season Forecast — Team Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast team performance for next season using historical average profiles\n",
    "current_teams = df[df['Season'] == '2024-25']['Team'].unique()\n",
    "forecast_rows = []\n",
    "for team in current_teams:\n",
    "    hist = df[df['Team'] == team]\n",
    "    if hist.empty:\n",
    "        continue\n",
    "    row = {\n",
    "        'Team': team,\n",
    "        'Team_encoded': hist['Team_encoded'].iloc[0],\n",
    "        'Season_encoded': 26  # hypothetical code for 2025-26\n",
    "    }\n",
    "    for col in feature_cols[2:]:  # skip encodings\n",
    "        row[col] = hist[col].mean()\n",
    "    row['Seasons_Used'] = len(hist)\n",
    "    forecast_rows.append(row)\n",
    "forecast_features = pd.DataFrame(forecast_rows)\n",
    "X_forecast = forecast_features[feature_cols]\n",
    "\n",
    "# Predict all targets\n",
    "forecast_preds = {t: xgb_models[t].predict(X_forecast) for t in target_cols}\n",
    "forecast_df = pd.DataFrame({'Team': forecast_features['Team'], **forecast_preds})\n",
    "forecast_df = forecast_df.sort_values('Points', ascending=False if 'Points' in forecast_df.columns else True).reset_index(drop=True)\n",
    "\n",
    "print(\"PREDICTED TEAM PERFORMANCE — 2025-26\")\n",
    "print(\"=\"*100)\n",
    "cols_to_show = ['Team'] + [t for t in ['Points','Points_Per_Game','Wins','Goals_Scored','Goals_Conceded','Goal_Difference','Win_Rate'] if t in forecast_df.columns]\n",
    "print(forecast_df[cols_to_show].to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nInsights:\")\n",
    "if 'Points' in forecast_df.columns:\n",
    "    print(' Top projected teams (by Points):', ', '.join(forecast_df.head(5)['Team']))\n",
    "if 'Goals_Scored' in forecast_df.columns:\n",
    "    top_attack = forecast_df.sort_values('Goals_Scored', ascending=False).head(5)['Team']\n",
    "    print(' Best projected attacks:', ', '.join(top_attack))\n",
    "if 'Goals_Conceded' in forecast_df.columns:\n",
    "    best_def = forecast_df.sort_values('Goals_Conceded').head(5)['Team']\n",
    "    print(' Best projected defenses (fewest GA):', ', '.join(best_def))\n",
    "\n",
    "print(\"\\nMethod: historical averages -> model(s) -> per-team metric forecasts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157dc6a",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary – XGBoost for Team Performance\n",
    "\n",
    "### What we predict\n",
    "- Points, Points per Game, Wins, Goals Scored/Conceded, Goal Difference, Win Rate\n",
    "\n",
    "### Workflow\n",
    "1. Load engineered team-season data  \n",
    "2. Time-aware split (train: historical, test: 2024-25)  \n",
    "3. Randomized search on anchor target (default: Points per Game)  \n",
    "4. Train one model per target with early stopping and refit  \n",
    "5. Evaluate per-target MAE / RMSE / R²  \n",
    "6. Interpret (Gain/Weight importance + optional SHAP)  \n",
    "7. 2024-25 predictions (actual vs predicted)  \n",
    "8. 2025-26 team performance forecast\n",
    "\n",
    "### Regularization & Overfitting Control\n",
    "- Structural: max_depth, min_child_weight, gamma  \n",
    "- Stochastic: subsample, colsample_bytree  \n",
    "- Penalty: reg_alpha (L1), reg_lambda (L2)  \n",
    "- Procedural: early stopping on validation MAE\n",
    "\n",
    "### Outputs\n",
    "- Per-target metrics table (train/test)  \n",
    "- Per-team predicted metrics for 2024-25  \n",
    "- Next-season (2025-26) forecast table  \n",
    "- Feature importance for the anchor target\n",
    "\n",
    "### Next steps\n",
    "- Calibrate uncertainty (quantile GBR / conformal intervals)  \n",
    "- Add domain priors (home/away splits, rolling form)  \n",
    "- Ensembling with RF/LightGBM for incremental lift  \n",
    "- Promotion team adjustment via similarity to nearest historical peers\n",
    "\n",
    "Artifacts: `xgb_models` (dict of trained models), `metrics_df`, `results`, `forecast_df`.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
